{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czNoNwKuGR0R"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "nxW8RZYrPs8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "\n",
        "from exai_tutorial import get_esol_data, binary_accuracy\n",
        "from graphrepr import featurise_data, feature_meaning\n",
        "from cyp_train import Net, train_model, sign_accuracy, best_ranked_accuracy, mse_masked_loss"
      ],
      "metadata": {
        "id": "wSIjoFSkGW3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device}\")"
      ],
      "metadata": {
        "id": "HiJ0-xSSZsTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1** Node-level regression task\n",
        "\n",
        "In this section, we will focus on a neural network that predicts Crippen contributions (regression) for each atom (node-level task)."
      ],
      "metadata": {
        "id": "uHKPa78OR3Sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation\n",
        "We will use ESOL data but instead of predicting solubility, we will calculate Crippen contributions for each atom and use them as labels."
      ],
      "metadata": {
        "id": "yRuFynWlZru9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "esol = get_esol_data()\n",
        "display(esol)"
      ],
      "metadata": {
        "id": "w-BbF_BEGWrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = featurise_data(esol, node_level=True, device=device)\n",
        "num_node_features = data_list[0].x.shape[1]\n",
        "print(f'Number of features: {num_node_features}')\n",
        "print(data_list[0])"
      ],
      "metadata": {
        "id": "eQDVs8aBQHwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# primitive train-test split\n",
        "num_train = 900\n",
        "train_loader = DataLoader(data_list[:num_train], batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(data_list[num_train:], batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "2GQ13CnrZsz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN definition and training\n",
        "\n",
        "We will use a standard GCN model that has only one convolutional layer."
      ],
      "metadata": {
        "id": "TWTxkluyRzFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building and training neural network\n",
        "model_name = 'model_regression.p'\n",
        "\n",
        "model = Net(hidden_size=512, num_node_features=num_node_features, num_classes=1,\n",
        "            num_conv_layers=1, num_linear_layers=5, dropout=0.5,\n",
        "            conv_layer='GCN', skip_connections=False, batch_norm=True, dummy_size=0, device=device).to(device)\n",
        "\n",
        "\n",
        "if os.path.isfile(model_name):\n",
        "  model.load_state_dict(torch.load(model_name, weights_only=True))\n",
        "  model.eval()\n",
        "  print('loaded a model')\n",
        "else:\n",
        "  n_epochs = 150  # ITCO CPU 100\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "  test_loss, epoch, metrics = train_model(model, train_loader, test_loader,\n",
        "                                           optimizer, n_epochs=n_epochs,\n",
        "                                           metrics={\n",
        "                                               'sign_acc': (sign_accuracy, 'graph'),\n",
        "                                               'mse': (mse_masked_loss, 'graph'),\n",
        "                                               },\n",
        "                                           device=device, model_path=model_name)\n",
        "\n",
        "  print(f'\\n trained a new model in {epoch} epochs and reached loss of {float(test_loss):.4f}')\n",
        "  print(f'test scores: {metrics}')"
      ],
      "metadata": {
        "id": "2NRv1DW1KQiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyse model's errors!\n",
        "\n",
        "We will calculate average mean square errors for each atom and visualise them."
      ],
      "metadata": {
        "id": "6QsVpPXJ7QP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(model, data_loader):\n",
        "  loss = []\n",
        "  for mol in data_loader:\n",
        "    pred = model.forward(data=mol)\n",
        "    loss.extend([mse_masked_loss(pred, mol.y).detach().cpu().numpy(),] * mol.num_nodes)\n",
        "\n",
        "  loss = np.array(loss)\n",
        "  print(f'MSE loss: {np.mean(loss)}')\n",
        "  return loss\n",
        "\n",
        "train_loss = mse(model, train_loader)\n",
        "test_loss = mse(model, test_loader)"
      ],
      "metadata": {
        "id": "wBLoBXskTL7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tetr = np.concatenate((test_loss, train_loss))\n",
        "maximum = sorted(tetr)[int(0.9*len(tetr))]  # 90% of errors are smaller/equal than this\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.subplot(121)\n",
        "plt.hist(train_loss, range=(np.min(tetr), np.max(tetr)), bins=120)\n",
        "plt.title('train errors')\n",
        "plt.subplot(122)\n",
        "plt.hist(test_loss, range=(np.min(tetr), np.max(tetr)), bins=120)\n",
        "plt.title('test errors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uWEYOy_HT0m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conclusion:**"
      ],
      "metadata": {
        "id": "iddJHWlk7kLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability\n",
        "\n",
        "Let's get accustomed to GNNExplainer explanations!"
      ],
      "metadata": {
        "id": "DbT3LN-QIfuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>First, we must define an explainer. Try figuring out what parameter values to use.<mark>\n",
        "\n",
        "- [Explainer documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/explain.html#explainer)\n",
        "- [GNNExplainer documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.explain.algorithm.GNNExplainer.html#torch_geometric.explain.algorithm.GNNExplainer)"
      ],
      "metadata": {
        "id": "Q_xxRP3Ge2ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = Explainer(\n",
        "    model=model,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',     # Explains the model prediction.\n",
        "    node_mask_type='attributes',  # Will mask each feature across all nodes.\n",
        "    edge_mask_type='object',      # Will mask each edge.\n",
        "    model_config=dict(\n",
        "        mode='regression',\n",
        "        task_level='node',\n",
        "        return_type='raw',  # not probabilities or log-probabilities\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "quqZ9hv0SdsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate an explanation for some sample.\n",
        "\n",
        "Since the GNN made a prediction for each atom separately, we must provide index of the atom whose prediction we're interested in."
      ],
      "metadata": {
        "id": "B6lXiLid8B9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_list[-5]\n",
        "node_index = 1\n",
        "\n",
        "expl = explainer(data.x, data.edge_index, index=node_index)\n",
        "print(f'Generated explanations in {expl.available_explanations}')"
      ],
      "metadata": {
        "id": "oMzK3IXiSdoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is how the explanation object looks like:')\n",
        "print(expl)"
      ],
      "metadata": {
        "id": "lO1JAG5lrPSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Let's write several functions that will help us analyse the explanations.</mark>\n",
        "\n",
        "Remember to call `cpu().numpy()` before using `torch.Tensor`s as input to numpy functions.\n",
        "\n",
        "`important_atom_features:`\n",
        "- **input:** explanation\n",
        "- **output:** indices of atom features with nonzero importance as `np.array[n_features, 2]`  with `(node index, feature index)` in each row\n",
        "- `np.nonzero` will be useful\n",
        "\n",
        "`neighbours:`\n",
        "- **input:** explanation, index of atom whose prediction was explained\n",
        "- **output:** indices of direct neighbours of the atom whose prediction was explained as `np.array`\n",
        "\n",
        "`important_features_indices:`\n",
        "- **input:** explanation\n",
        "- **output:** indices of all features which had nonzero importance for at least one atom\n",
        "- `np.where` will be useful\n",
        "\n",
        "`important_edges:`\n",
        "- **input:** explanation\n",
        "- **output:** a list of edges with nonzero importance as `np.array[2, n_edges]` `(begin node index, end node index)`"
      ],
      "metadata": {
        "id": "B6tVtdGsfVTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this will be written by students\n",
        "def important_atom_features(explanation):\n",
        "  return np.nonzero(explanation.node_mask).cpu().numpy()  # node index x feature index\n",
        "\n",
        "def neighbours(explanation, node_id):\n",
        "  # check if nodes with important features are those that are close enough to the node that is being explained\n",
        "  adj = explanation.edge_index.cpu().numpy()\n",
        "  which_nodes = adj[1, adj[0,:]==node_id]  # this should give us all nodes directly connected to node at `node_id`\n",
        "  return which_nodes\n",
        "\n",
        "def important_features_indices(explanation):\n",
        "  feature_importance = np.sum(explanation.node_mask.cpu().numpy(), axis=0)\n",
        "  which_features = np.where(feature_importance>0)  # indices of important features\n",
        "  return which_features\n",
        "\n",
        "def important_edges(explanation):\n",
        "  return explanation.edge_index.cpu().numpy()[:, explanation.edge_mask.cpu().numpy()>0]   # te krawędzie były ważne\n"
      ],
      "metadata": {
        "id": "DnE9h1EFbkXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_features = important_atom_features(expl)\n",
        "neigh_nodes = neighbours(expl, node_index)\n",
        "imp_edges = important_edges(expl)"
      ],
      "metadata": {
        "id": "TnXCywV9Lc4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Sanity check</mark>\n",
        "Check if nodes with important features are those that are close enough to the node that is being explained."
      ],
      "metadata": {
        "id": "OKzi6x9H81Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for node in set(imp_features[:,0]):\n",
        "  print(node, node in neigh_nodes.tolist() + [node_index,])"
      ],
      "metadata": {
        "id": "tUCKP_D5Lc1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conclusion:**"
      ],
      "metadata": {
        "id": "7jCDl9fh8-cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which features are important (their meaning)?"
      ],
      "metadata": {
        "id": "07mS16xZ9BP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_meaning[important_features_indices(expl)]"
      ],
      "metadata": {
        "id": "MjtzT55BLcyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Is there a tendency for important features to have a certain feature value (zero or one)?</mark>\n",
        "\n",
        "- `np.where` will be useful again"
      ],
      "metadata": {
        "id": "U8ZA03KN9OzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these should be sets\n",
        "nonzero_features = np.where(expl.x.cpu().numpy()!=0)\n",
        "nonzero_features = set(zip(*nonzero_features))\n",
        "\n",
        "important_features = set(zip(*imp_features.T))\n",
        "\n",
        "print(important_features - nonzero_features)"
      ],
      "metadata": {
        "id": "sXk3xuMVLcwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conclusion:**"
      ],
      "metadata": {
        "id": "szy9sVYvfHCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualising explanations"
      ],
      "metadata": {
        "id": "e-09IfOMhHup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit.Chem import MolFromSmiles\n",
        "from rdkit.Chem.Draw import rdMolDraw2D\n",
        "from IPython.display import SVG\n",
        "import io\n",
        "from PIL import Image\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "QdXxsSi2gnMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Let's write a few functions that will help us with visualisations.</mark>\n",
        "\n",
        "`important_nodes:`\n",
        "- **input:** explanation\n",
        "- **output:** indices of all nodes for which at least one feature had nonzero importance as a `list`\n",
        "- use `np.nonzero`\n",
        "\n",
        "`node_importance:`\n",
        "- **input:** explanation\n",
        "- **output:** importance of each node defined as a sum of importance scores of it's features as a `np.array[n_nodes]`\n",
        "- use `np.sum`\n",
        "\n",
        "`edge_importance:`\n",
        "- **input:** explanation\n",
        "- **output:** importance of each bond defined analogously as above as a `dict` with keys `(start node index, end node index)`\n",
        "- mind that each bond appears twice in `edge_mask`"
      ],
      "metadata": {
        "id": "rJCFXEnviJ9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def important_nodes(explanation):\n",
        "  # which nodes were important\n",
        "  return np.nonzero(np.sum(expl.node_mask.cpu().numpy(), axis=1))[0].tolist()\n",
        "\n",
        "def node_importance(explanation):\n",
        "  # how important each node is\n",
        "  return np.sum(expl.node_mask.cpu().numpy(), axis=1).astype(float)\n",
        "\n",
        "def edge_importance(explanation):\n",
        "  bond_importance = defaultdict(float)\n",
        "  for e1, e2, imp in list(zip(*explanation.edge_index.cpu(), explanation.edge_mask.cpu())):\n",
        "    start = np.min((e1, e2))\n",
        "    end = np.max((e1, e2))\n",
        "    bond_importance[(int(start), int(end))] += float(imp)\n",
        "  return dict(bond_importance)"
      ],
      "metadata": {
        "id": "QLc9Xd61ixY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualise everything\n",
        "Our first function will visualise:\n",
        "- for which node the explanation was calculated\n",
        "- what are it's neighbours\n",
        "- how important each atom is\n",
        "- how important each bond is"
      ],
      "metadata": {
        "id": "th5ewajZ8cpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise_everything(sample, explanation):\n",
        "  mol = sample.mol\n",
        "\n",
        "  no_col = (0.0, 0.0, 0.0, 0.0)\n",
        "  rgba_color = (0.0, 0.0, 1.0, 0.5) # transparent blue for node being explained\n",
        "  neigh_col = (0.8, 0.0, 0.8, 0.5)  # purple for direct neighbours\n",
        "  imp_col = (0.0, 0.8, 0.0)    # green for nodes with non-zero importance\n",
        "  bonds_col = (0.8, 0.8, 0.8)  # gray for bonds\n",
        "\n",
        "  imp_nodes = important_nodes(explanation)       # which atoms are important\n",
        "  atom_importance = node_importance(explanation) # how important each atom is\n",
        "\n",
        "  # atoms that are neighbour to the node being explained\n",
        "  neighs = neighbours(explanation, explanation.index).tolist() if hasattr(explanation, 'index') else []\n",
        "  imp_edges = important_edges(explanation)  # edges that are important\n",
        "  bond_importance = edge_importance(explanation)  # how important each edge is\n",
        "  bond_normalisation = np.max(list(bond_importance.values()))\n",
        "\n",
        "  atom_highlights = defaultdict(list)  # higlight colours for each atom\n",
        "  arads = {}                           # highlight size (here: atom importance)\n",
        "\n",
        "  # colouring atoms\n",
        "  for a in mol.GetAtoms():\n",
        "    a_idx = a.GetIdx()\n",
        "    colours = []\n",
        "\n",
        "    if hasattr(explanation, 'index') and a_idx == explanation.index:\n",
        "      # node being explained\n",
        "      colours.append(rgba_color)\n",
        "    if a_idx in neighs:\n",
        "      # its neighbours\n",
        "      colours.append(neigh_col)\n",
        "    if a_idx in imp_nodes:\n",
        "      # nodes with non-zero importance\n",
        "      colours.append(imp_col + (float(atom_importance[a_idx]/np.max(atom_importance)), ) )\n",
        "\n",
        "    if len(colours)==0:\n",
        "      # other atoms (each atom must be defined)\n",
        "      colours.append(no_col)\n",
        "\n",
        "    atom_highlights[a_idx].extend(colours)\n",
        "    arads[a_idx] = float(atom_importance[a_idx])\n",
        "\n",
        "  # colouring bonds\n",
        "  bond_highlights = defaultdict(list)\n",
        "  imp_bonds = [mol.GetBondBetweenAtoms(int(edge[0]), int(edge[1])).GetIdx() for edge in list(zip(*imp_edges))]\n",
        "\n",
        "  for bond in mol.GetBonds():\n",
        "      bid = bond.GetIdx()\n",
        "      b1, b2 = min(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()), max(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())\n",
        "\n",
        "      if bid in imp_bonds:\n",
        "          bond_highlights[bid].append(bonds_col + (float(bond_importance[(b1, b2)]/bond_normalisation), ))\n",
        "      else:\n",
        "          bond_highlights[bid].append(no_col)\n",
        "\n",
        "  arads = dict()\n",
        "\n",
        "  # making a drawing\n",
        "  d = rdMolDraw2D.MolDraw2DSVG(400, 200) # MolDraw2DSVG for SVG or MolDraw2DCairo to get PNGs\n",
        "  d.DrawMoleculeWithHighlights(mol, sample.smiles, dict(atom_highlights), dict(bond_highlights), arads, {})\n",
        "  d.FinishDrawing()\n",
        "\n",
        "  return d\n"
      ],
      "metadata": {
        "id": "AmGbIgDNsQUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = visualise_everything(data, expl)\n",
        "SVG(d.GetDrawingText())"
      ],
      "metadata": {
        "id": "YvF2Sp_iHS6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conclusion:**"
      ],
      "metadata": {
        "id": "g_KJ4wOEm7eM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualise importance only\n",
        "\n",
        "Our second function will colour atoms based on their importance for the prediction."
      ],
      "metadata": {
        "id": "__RlW46U8i_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise_importance(sample, explanation):\n",
        "  mol = sample.mol\n",
        "  red = (1, 0, 0.3)\n",
        "\n",
        "  atom_highlights = defaultdict(list)  # higlight colours for each atom\n",
        "  arads = {}                           # highlight size\n",
        "\n",
        "  atom_importance = node_importance(explanation) # how important each atom is\n",
        "\n",
        "  for a in mol.GetAtoms():\n",
        "    a_idx = a.GetIdx()\n",
        "\n",
        "    col = red + (atom_importance[a_idx]/np.max(atom_importance),)\n",
        "    atom_highlights[a_idx].append(col)\n",
        "\n",
        "  d = rdMolDraw2D.MolDraw2DSVG(400, 200) # MolDraw2DSVG for SVG or MolDraw2DCairo to get PNGs\n",
        "  d.DrawMoleculeWithHighlights(mol, sample.smiles, dict(atom_highlights), dict(), {}, {})\n",
        "\n",
        "  d.FinishDrawing()\n",
        "\n",
        "  return d"
      ],
      "metadata": {
        "id": "cv6HKFRfaWL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = visualise_importance(data, expl)\n",
        "SVG(d.GetDrawingText())  # SVG"
      ],
      "metadata": {
        "id": "Z8yjkscUgtZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Play time!</mark>\n",
        "\n",
        "Now let's look at explanations for some molecules. Do they make sense from a chemical point of view?"
      ],
      "metadata": {
        "id": "zgpNeJ8akhiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample = np.random.randint(len(data_list))\n",
        "data = data_list[random_sample]\n",
        "node_index = np.random.randint(data.num_nodes)\n",
        "\n",
        "expl = explainer(data.x, data.edge_index, index=node_index)\n",
        "print(f'Molecule index: {random_sample}, atom index: {node_index}')\n",
        "print(f'Generated explanations in {expl.available_explanations}')"
      ],
      "metadata": {
        "id": "fqxAsjZOgtVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.mol"
      ],
      "metadata": {
        "id": "cMlgS5CuGws-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = visualise_everything(data, expl)\n",
        "SVG(d.GetDrawingText())  # SVG"
      ],
      "metadata": {
        "id": "Pk6JGHhAmCn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = visualise_importance(data, expl)\n",
        "SVG(d.GetDrawingText())  # SVG"
      ],
      "metadata": {
        "id": "fuaqOakTjyne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKe3twvymCkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2** Graph-level classification task\n",
        "\n",
        "In this section, we will focus on a neural network that predicts if a molecule (graph-level task) is soluble (classification).\n",
        "\n",
        "We will use ESOL data and classify molecules as having solubility higher or lower than **`-3`**."
      ],
      "metadata": {
        "id": "axviU5j8wAar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation"
      ],
      "metadata": {
        "id": "4tyDWI_Tlsci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = featurise_data(esol, node_level=False, device=device)\n",
        "num_node_features = data_list[0].x.shape[1]\n",
        "print(f'Number of features: {num_node_features}')\n",
        "print(data_list[0])\n",
        "\n",
        "# primitive train-test split\n",
        "num_train = 900\n",
        "train_loader = DataLoader(data_list[:num_train], batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(data_list[num_train:], batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "8ew3tFR6aV8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition and training\n",
        "\n",
        "This time, we will use a GCN with several convolutional layers that makes a prediction for the entire molecule."
      ],
      "metadata": {
        "id": "AaqL8oyYlv27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building and training neural network\n",
        "model_name = 'model_classification.p'\n",
        "\n",
        "model = Net(hidden_size=512, num_node_features=num_node_features, num_classes=2,\n",
        "            num_conv_layers=3, num_linear_layers=3, dropout=0.5,\n",
        "            conv_layer='GCN', skip_connections=False, batch_norm=True, dummy_size=0,\n",
        "            graph_level=True, device=device).to(device)\n",
        "\n",
        "if os.path.isfile(model_name):\n",
        "  model.load_state_dict(torch.load(model_name, weights_only=True))\n",
        "  model.eval()\n",
        "  print('loaded a model')\n",
        "else:\n",
        "  n_epochs = 40  # 30 ITCO CPU\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "\n",
        "  test_loss, epoch, metrics = train_model(model, train_loader, test_loader, optimizer,\n",
        "                                          n_epochs=n_epochs, model_path=model_name,\n",
        "                                          metrics={'acc': (binary_accuracy, 'graph'),},\n",
        "                                          device=device)\n",
        "\n",
        "  print(f'\\n trained a new model in {epoch} epochs and reached loss of {float(test_loss):.4f}')\n",
        "  print(f'test scores: {metrics}')"
      ],
      "metadata": {
        "id": "UHJco7HTaV4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "for batch in train_loader:\n",
        "  acc += binary_accuracy(model(batch), batch.y) * batch.num_graphs\n",
        "\n",
        "acc = acc/len(train_loader.dataset)\n",
        "print(f'Train accuracy: {acc:.4f}')\n"
      ],
      "metadata": {
        "id": "ag3J6x5YRtbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what for molecules the model makes errors."
      ],
      "metadata": {
        "id": "pnZaa77i_eRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyse_mispredictions(model, loader, print_func=lambda x:None):\n",
        "  smis, vals, preds = [], [], []\n",
        "  for batch in loader:\n",
        "    pred = model(batch)\n",
        "    pred_class = pred[:, 0] < pred[:, 1]\n",
        "    mask = (pred_class!=batch.y).cpu().numpy()\n",
        "\n",
        "    smis.extend(np.array(batch.smiles)[mask])\n",
        "    vals.extend(batch.raw_y[mask])\n",
        "    preds.extend(pred[mask])\n",
        "\n",
        "  for smi, val, p in zip(smis, vals, preds):\n",
        "    print_func(f'true: {float(val):.2f}  {str(smi)} ')\n",
        "\n",
        "  return smis, np.array(vals), preds\n"
      ],
      "metadata": {
        "id": "yaCT3IjOcYxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_smis, tr_vals, tr_preds = analyse_mispredictions(model, train_loader)\n",
        "te_smis, te_vals, te_preds = analyse_mispredictions(model, test_loader)\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle(\"solubility of mispredicted molecules from the...\")\n",
        "plt.subplot(121)\n",
        "plt.hist(tr_vals, bins=15)\n",
        "plt.plot([-3, -3], [0, max(plt.yticks()[0])])\n",
        "plt.title(f\"train set ({len(tr_vals)} = {np.sum(tr_vals<=-3)} + {np.sum(tr_vals>-3)})\")\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.hist(te_vals, bins=15)\n",
        "plt.plot([-3, -3], [0, max(plt.yticks()[0])])\n",
        "plt.title(f\"test set ({len(te_vals)} = {np.sum(te_vals<=-3)} + {np.sum(te_vals>-3)})\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mw7_ybhwt7P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do mispredicted molecules come from both classes equally or are they mostly from one of the classes?\n",
        "\n",
        "Is is the same for train and test data?\n",
        "\n",
        "**conclusion:**"
      ],
      "metadata": {
        "id": "bW50XVyFgdA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is there a correlation between train/test data and solubility value?"
      ],
      "metadata": {
        "id": "dEsfK5wPCLGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = [d.raw_y for d in data_list]\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.scatter(list(range(len(y))), y, s=5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-UuZpBPegb3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conclusion:**"
      ],
      "metadata": {
        "id": "i8IfvXNQCQ9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability"
      ],
      "metadata": {
        "id": "TRemGxyOmAb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>What should be the parameter values in this case?</mark>"
      ],
      "metadata": {
        "id": "vlxPva9hmFmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = Explainer(\n",
        "    model=model,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model', # Explains the model prediction.\n",
        "    node_mask_type='object',  # Will mask each node.\n",
        "    edge_mask_type='object',  # Will mask each edge.\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='graph',\n",
        "        return_type='log_probs',  # the model returns log-probabilities\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "RNxw7YFkxxbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>Play time!</mark>\n",
        "\n",
        "Let's have a look at some explanations. Does the model looks where it should?"
      ],
      "metadata": {
        "id": "4l1mPVRPmOSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_index = np.random.randint(len(data_list))\n",
        "data = data_list[random_index]\n",
        "mol =  data.mol\n",
        "expl = explainer(data.x, data.edge_index)\n",
        "print(f'Molecule index: {random_index}')\n",
        "print(f'Generated explanations in {expl.available_explanations}')\n",
        "print(expl)"
      ],
      "metadata": {
        "id": "mg6MNwaq84j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mol"
      ],
      "metadata": {
        "id": "IlvwGOLQ6Y_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = visualise_importance(data, expl)\n",
        "SVG(d.GetDrawingText())"
      ],
      "metadata": {
        "id": "7wZMvIn1gNxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = visualise_everything(data, expl)\n",
        "SVG(d.GetDrawingText())"
      ],
      "metadata": {
        "id": "2GoUJ658UMwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZsOz2t-gNsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyse the influence of the number of epochs on explanations produced by GNNExplainer.\n",
        "\n",
        "Just like (almost) any other method in ML, GNNExplainer has hyperparametrs whose values must be carefully chosen. In this case, it is the number of epochs for which the GNNExplainer model is trained."
      ],
      "metadata": {
        "id": "nYaflh_FmcpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>What should be the parameter values in this case?</mark>"
      ],
      "metadata": {
        "id": "ynYEmQvyAP4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer50 = Explainer(\n",
        "    model=model,\n",
        "    algorithm=GNNExplainer(epochs=50),\n",
        "    explanation_type='model', # Explains the model prediction.\n",
        "    node_mask_type='object',  # Will mask each node.\n",
        "    edge_mask_type='object',  # Will mask each edge.\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='graph',\n",
        "        return_type='log_probs',  # the model returns log-probabilities\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Pc88BnXsbmiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e200 = [explainer(data.x, data.edge_index) for i in range(10)]\n",
        "e50 = [explainer50(data.x, data.edge_index) for i in range(10)]"
      ],
      "metadata": {
        "id": "Rtr8fDaT84db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nm200 = np.array([e.node_mask.cpu() for e in e200])\n",
        "nm50 = np.array([e.node_mask.cpu() for e in e50])\n",
        "\n",
        "nv200 = np.std(nm200, axis=0)\n",
        "nv50 = np.std(nm50, axis=0)\n",
        "\n",
        "for var50, var200 in zip(nv50[:,0], nv200[:,0]):\n",
        "  print(f'{var50:.4f} {var200:.4f}')"
      ],
      "metadata": {
        "id": "fXhyn572WLzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conclusion:**"
      ],
      "metadata": {
        "id": "-KQjgaehCj2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "em200 = np.array([e.edge_mask.cpu() for e in e200])\n",
        "em50 = np.array([e.edge_mask.cpu() for e in e50])\n",
        "\n",
        "ev200 = np.std(em200, axis=0)\n",
        "ev50 = np.std(em50, axis=0)\n",
        "\n",
        "for var50, var200 in zip(ev50, ev200):\n",
        "  print(f'{var50:.4f} {var200:.4f}')"
      ],
      "metadata": {
        "id": "cwtAz1hnbZaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conclusion:**"
      ],
      "metadata": {
        "id": "oKByfXbiCmx4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RtVj4jatcZtg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}